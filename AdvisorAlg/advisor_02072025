# %% Import


#make this a tester for checking

import os
import time
from enum import Enum
import pandas as pd
import numpy as np
import pyomo.environ as pyo

from pyomo.common.fileutils import this_file_dir
from pyomo.common.collections import ComponentSet, ComponentMap
from pyomo.util.calc_var_value import calculate_variable_from_constraint
# import idaes
# import idaes.core.util.scaling as iscale
from pyomo.dae import ContinuousSet, DerivativeVar
# from idaes.core.solvers import petsc
# import idaes.logger as idaeslog
# import idaes.core.util.model_serializer as ms

import matplotlib.pyplot as plt
import matplotlib.ticker as mtick

from pyomo.dae.flatten import flatten_dae_components
from scipy.io import loadmat

import random
import copy as copy
import torch
import torch.nn as nn
import math
from matplotlib import pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
import pickle
#################################################################################
from matplotlib.font_manager import FontProperties
import numpy as np
from numpy import random as rand
from matplotlib import pyplot as plt
import copy as copy


import numpy as np
from matplotlib import pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
# from idaes.core.util.model_statistics import degrees_of_freedom
from pyomo.environ import *
from pyomo.environ import SolverFactory
from pyomo.dae import *
from pyomo.dae.simulator import Simulator
from idaes.core import FlowsheetBlock
import idaes.logger as idaeslog
import copy as copy
import os
import time
from enum import Enum
import pandas as pd
import numpy as np
import pyomo.environ as pyo
# from pyomo.repn.plugins import nl_writer
# nl_writer._activate_nl_writer_version(2)
from pyomo.common.fileutils import this_file_dir
from pyomo.common.collections import ComponentSet, ComponentMap
from pyomo.util.calc_var_value import calculate_variable_from_constraint
# import idaes
# import idaes.core.util.scaling as iscale
from pyomo.dae import ContinuousSet, DerivativeVar
from idaes.core.solvers import petsc
import idaes.logger as idaeslog
import idaes.core.util.model_serializer as ms


import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
from idaes.models.control.controller import ControllerType, ControllerMVBoundType, ControllerAntiwindupType

from idaes.models.properties import iapws95

from pyomo.dae.flatten import flatten_dae_components
from scipy.io import loadmat
import random
import copy as copy


from matplotlib import pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
import pickle
from sklearn import mixture

# %% RL functions


def mov_med(rcat,mds_=60*3): #Calculate moving median
    ccc=[]
    mds=mds_
    for i in range(len(rcat)):
        if i<mds:
            ccc.append(np.median(rcat[0:i+1]))
        else:
            ccc.append(np.median(rcat[i-mds:i]))
    return ccc

def mov_mean(rcat,mds_=60*3): #Calculate moving mean
    ccc=[]
    mds=mds_
    for i in range(len(rcat)):
        if i<mds:
            ccc.append(np.mean(rcat[0:i+1]))
        else:
            ccc.append(np.mean(rcat[i-mds:i]))
    return ccc

class scale_fun_htan:
    def __init__(self,low,high):
        self.high=high
        self.low=low

        self.ka=(self.high-self.low)/2
        self.ba=(self.high+self.low)/2
    def act(self, anorm):
        aact=self.ka*anorm+self.ba
        return aact    
    def norm(self, aact):
        anorm=(aact-self.ba)/self.ka
        return anorm
    
class scale_fun_sig:
    def __init__(self,low,high):
        self.high=high
        self.low=low

        self.ka=(self.high-self.low)/1
        self.ba=self.low
    def act(self, anorm):
        aact=self.ka*anorm+self.ba
        return aact    
    def norm(self, aact):
        anorm=(aact-self.ba)/self.ka
        return anorm

def create_scales_sig(lbs,ubs):   #Scale variables 0 to 1
    h=[]
    for i in range(len(lbs)):
        h.append(scale_fun_sig(lbs[i],ubs[i]))
    return h

def create_scales_tanh(lbs,ubs):   #Scale variables -1 to 1
    h=[]
    for i in range(len(lbs)):
        h.append(scale_fun_htan(lbs[i],ubs[i]))
    return h


class Buffer:    #standard buffer for actor-critc RL semi batch udpate
    def __init__(self, n_states=1, n_actions=1, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0

        
        self.state_buffer = []
        self.action_buffer = []
        self.reward_buffer = []
        self.next_state_buffer = []
        self.on = 0
        # self.state_buffer = np.zeros((self.buffer_capacity, n_states))
        # self.action_buffer = np.zeros((self.buffer_capacity, n_actions))
        # self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        # self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))

        self.state_buffer_s = []
        self.action_buffer_s = []
        self.reward_buffer_s = []
        self.next_state_buffer_s = []

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity
        
        if self.buffer_counter > 0 and index==0:
            self.on=1
        if self.on==1:
            self.state_buffer[index] = obs_tuple[0]
            self.action_buffer[index] = obs_tuple[1]
            self.reward_buffer[index] = obs_tuple[2]
            self.next_state_buffer[index] = obs_tuple[3]
        else:
            self.state_buffer.append(obs_tuple[0])
            self.action_buffer.append(obs_tuple[1])
            self.reward_buffer.append(obs_tuple[2])
            self.next_state_buffer.append(obs_tuple[3])

        self.buffer_counter += 1
        
    def get_batch(self):
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),self.batch_size)
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])
        return states, actions, rewards, next_states
    
    def get_batch1_2(self):  #modified buffer batch retirval
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size/2:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),self.batch_size/2)
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])
        return states, actions, rewards, next_states



class policy(nn.Module):   #pytorch policy structure
    def __init__(self,n_states,n_actions):
        super(policy, self).__init__()
        
        self.stack=nn.Sequential(nn.Linear(n_states,100),
                                    nn.ReLU(),
                                    # nn.ReLU(),
                                    # nn.ReLU(),
                                      nn.Linear(100, 75),
                                      nn.ReLU(),
                                    nn.Linear(75,n_actions),
                                    nn.Tanh()
                                    # nn.Sigmoid()
                                    )       
        

        # self.stack=nn.Sequential(nn.Linear(n_states,1280),
        #                       nn.ReLU(),
        #                       nn.Linear(1280, 128),
        #                       nn.ReLU(),
        #                       nn.Linear(128, 64),
        #                       nn.ReLU(),
        #                       nn.Linear(64,n_actions),
        #                       nn.Tanh() #nn.Sigmoid()
        #                       )       
        
    def forward(self,x):
        y=self.stack(x)
        return y
         
class Qfun(nn.Module):   #pytorch critic structure
    def __init__(self, n_states, n_actions):
        super(Qfun,self).__init__()
        
        self.stack1=nn.Sequential(nn.Linear(n_states,100),
                              nn.ReLU(),
                              nn.ReLU(),
                            #   nn.Sigmoid(),
                            #   nn.Linear(640, 128),
                            #   nn.ReLU(),
                              )
        
        
        self.stack2=nn.Sequential(nn.Linear(n_actions,100),
                              nn.ReLU(),
                            #   nn.ReLU(),
                            #   nn.Sigmoid(),
                            #   nn.Linear(640, 128),
                            #   nn.ReLU(),
                              )
        
        self.stack3=nn.Sequential(nn.Linear(100*2, 100),
                                  nn.ReLU(),
                                  nn.ReLU(),
                                  nn.Linear(100,1))
        


        # self.stack1=nn.Sequential(nn.Linear(n_states,640),
        #                       nn.ReLU(),
        #                       nn.Linear(640, 128),
        #                       nn.ReLU(),
        #                       )
        
        
        # self.stack2=nn.Sequential(nn.Linear(n_actions,640),
        #                       nn.ReLU(),
        #                       nn.Linear(640, 128),
        #                       nn.ReLU(),
        #                       )
        
        # self.stack3=nn.Sequential(nn.Linear(128*2, 64),
        #                           nn.ReLU(),
        #                           nn.Linear(64,1))
        
    def forward(self,s,a):
        s1=self.stack1(s)
        a1=self.stack2(a)
        z=torch.cat((s1,a1),len(s1.size())-1)
        # z=torch.cat((s1,a1),1)
        # z=torch.cat((s1,a1))
        out=self.stack3(z)
        return out

class DDPGagent:  #Deep determinsitic policy gradient
    def __init__(self, n_states=1, n_actions=1,  actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, buffer_capacity=100000, batch_size=64, w_decay=0):
        # Params
        self.num_states = n_states
        self.num_actions = n_actions
        self.gamma = gamma
        self.tau = tau

        self.c_L=[]
        self.p_L=[]
        self.Qcat=[]


        # Networks
        self.actor = policy(self.num_states,self.num_actions)
        self.actor_target = policy(self.num_states,self.num_actions)
        self.critic = Qfun(self.num_states, self.num_actions)
        self.critic_target = Qfun(self.num_states, self.num_actions)

        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data)

        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data)
        
        # Training
        self.RB = Buffer(self.num_states,self.num_actions,buffer_capacity)        
        self.critic_loss  = nn.MSELoss()
        self.actor_opt= torch.optim.Adam(self.actor.parameters(), lr=actor_learning_rate, weight_decay=w_decay)   #weight_decay
        self.critic_opt= torch.optim.Adam(self.critic.parameters(), lr=critic_learning_rate, weight_decay=w_decay)
    
        self.gmm=mixer()

    def get_action(self, state):
        state = torch.tensor([state]).float()
        action = self.actor.forward(state)
        action = np.squeeze(action.detach().numpy())
        # action=np.ndarray.item(action)
        return action
    
    def update(self):
        states, actions, rewards, next_states = self.RB.get_batch()
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    
        # Critic loss        
        Qvals = self.critic.forward(states, actions)
        next_actions = self.actor_target.forward(next_states)
        next_Q = self.critic_target.forward(next_states, next_actions.detach())
        Qprime = rewards + self.gamma * next_Q
        critic_loss = self.critic_loss(Qvals, Qprime)
        self.c_L.append(critic_loss.detach().numpy())

        self.Qcat.append(Qvals.mean().item())

        # Actor loss
        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()
        self.p_L.append(policy_loss.detach().numpy())

        # update networks
        self.actor_opt.zero_grad()
        policy_loss.backward()
        self.actor_opt.step()

        self.critic_opt.zero_grad()
        critic_loss.backward() 
        self.critic_opt.step()

        # update target networks 
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))


class TD3:   #twin delayed deterministic policy gradient
    def __init__(self, n_states=1, n_actions=1,  actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, buffer_capacity=100000, batch_size=64, w_decay=0, p=2):
        # Params
        self.num_states = n_states
        self.num_actions = n_actions
        self.gamma = gamma
        self.tau = tau

        self.batch_size=batch_size

        self.c_L1=[]
        self.c_L2=[]
        self.p_L=[]

        self.m=0
        self.p=p

        # Networks
        self.actor = policy(self.num_states,self.num_actions)
        self.actor_target = policy(self.num_states,self.num_actions)
        self.critic1 = Qfun(self.num_states, self.num_actions)
        self.critic_target1 = Qfun(self.num_states, self.num_actions)
        self.critic2 = Qfun(self.num_states, self.num_actions)
        self.critic_target2 = Qfun(self.num_states, self.num_actions)

        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data)

        for target_param, param in zip(self.critic_target1.parameters(), self.critic1.parameters()):
            target_param.data.copy_(param.data)

        for target_param, param in zip(self.critic_target2.parameters(), self.critic2.parameters()):
            target_param.data.copy_(param.data)
        
        # Training
        self.RB = Buffer(self.num_states,self.num_actions,buffer_capacity,batch_size)        
        self.critic_loss1  = nn.MSELoss()
        self.critic_loss2  = nn.MSELoss()
        self.actor_opt= torch.optim.Adam(self.actor.parameters(), lr=actor_learning_rate, weight_decay=w_decay)   #weight_decay
        self.critic_opt1= torch.optim.Adam(self.critic1.parameters(), lr=critic_learning_rate, weight_decay=w_decay)
        self.critic_opt2= torch.optim.Adam(self.critic2.parameters(), lr=critic_learning_rate, weight_decay=w_decay)

    def get_action(self, state):
        state = torch.tensor([state]).float()
        action = self.actor.forward(state)
        action = np.squeeze(action.detach().numpy())
        # action=np.ndarray.item(action)
        return action
    
    def update(self):
        states, actions, rewards, next_states = self.RB.get_batch()
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    


        # Critic loss        
        Qvals1 = self.critic1.forward(states, actions)
        Qvals2 = self.critic2.forward(states, actions)
        na_= self.actor_target.forward(next_states).detach()
        next_actions = np.clip(na_ + np.clip(np.random.normal(0,0.1,na_.size()),-.1,.1) ,-1,1).float()

        next_Q1 = self.critic_target1.forward(next_states, next_actions)
        next_Q2 = self.critic_target2.forward(next_states, next_actions)
        next_Q=torch.min(next_Q1,next_Q2)

        Qprime = rewards + self.gamma * next_Q.detach()
        critic_loss1 = self.critic_loss1(Qvals1, Qprime)
        self.c_L1.append(critic_loss1.detach().numpy())
        critic_loss2 = self.critic_loss2(Qvals2, Qprime)
        self.c_L2.append(critic_loss2.detach().numpy())

        #Critic Update
        self.critic_opt1.zero_grad()
        critic_loss1.backward() 
        self.critic_opt1.step()

        self.critic_opt2.zero_grad()
        critic_loss2.backward() 
        self.critic_opt2.step()

        if self.m%self.p==0:
            # Actor loss
            policy_loss = -self.critic1.forward(states, self.actor.forward(states)).mean()
            self.p_L.append(policy_loss.detach().numpy())

            # Actorupdate networks     

            self.actor_opt.zero_grad()
            policy_loss.backward()
            self.actor_opt.step()

            for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
                target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
            for target_param, param in zip(self.critic_target1.parameters(), self.critic1.parameters()):
                target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))

            for target_param, param in zip(self.critic_target2.parameters(), self.critic2.parameters()):
                target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
        

        self.m+=1

        # update target networks 
 

class OUActionNoise:   #ornstein uhlenbock process for exploration nosie
    def __init__(self, mean, sigma_max=.35, sigma_min=0, theta=0.15, dt=1, decay_period=1e4, x_initial=None):
        self.theta = theta
        self.mean = mean
        self.sigma = sigma_max
        self.max_sigma=sigma_max
        self.min_sigma=sigma_min
        self.dt = dt
        self.x_initial = x_initial
        self.decay_period=decay_period
        self.reset()

    def __call__(self,t):
        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.
        x = (
            self.x_prev
            + self.theta * (self.mean - self.x_prev) * self.dt
            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)
        )
        # Store x into x_prev
        # Makes next noise dependent on current one
        self.x_prev = x
        
        
        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)
        
        return x

    def reset(self):
        if self.x_initial is not None:
            self.x_prev = self.x_initial
        else:
            self.x_prev = np.zeros_like(self.mean)



# %%  BUffer



class Buffer2:   #modified buffer for gmm update and exp
    def __init__(self, n_states=1, n_actions=1, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0

        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        self.state_buffer = []
        self.action_buffer = []
        self.reward_buffer = []
        self.next_state_buffer = []
        self.on = 0
        # self.state_buffer = np.zeros((self.buffer_capacity, n_states))
        # self.action_buffer = np.zeros((self.buffer_capacity, n_actions))
        # self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        # self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))


        self.num_states=n_states
        self.num_actions=n_actions
        self.clf=[]
        self.data=[]
        self.pbL=[]

        self.state_buffer_s = []
        self.action_buffer_s = []
        self.reward_buffer_s = []
        self.next_state_buffer_s = []

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity
        
        if self.buffer_counter > 0 and index==0:
            self.on=1
        if self.on==1:
            self.state_buffer[index] = obs_tuple[0]
            self.action_buffer[index] = obs_tuple[1]
            self.reward_buffer[index] = obs_tuple[2]
            self.next_state_buffer[index] = obs_tuple[3]
        else:
            self.state_buffer.append(obs_tuple[0])
            self.action_buffer.append(obs_tuple[1])
            self.reward_buffer.append(obs_tuple[2])
            self.next_state_buffer.append(obs_tuple[3])

        self.buffer_counter += 1
            
    def generate(self, comp=6):

        del self.clf
        
        sb=self.state_buffer
        ab=self.action_buffer
        rb=self.reward_buffer
        ssb=self.next_state_buffer

        # sb2=np.concatenate(sb).squeeze(1)
        # ab2=np.concatenate(ab).squeeze(1)
        # rb2=np.concatenate(rb).squeeze(1)
        # ssb2=np.concatenate(ssb).squeeze(1)

        self.data=np.concatenate((sb,ab,rb,ssb),1)

        data2=self.data[:,0:Agent.num_states+Agent.num_actions]
        self.clf = mixture.GaussianMixture(n_components=comp, covariance_type="full")
        self.clf.fit(data2)

    def predict_set(self,x,a, comp=6):
        # self.generate(comp)

        label=self.clf.predict(np.concatenate((x.T,a),1))
        prb=self.clf.predict_proba(np.concatenate((x.T,a),1))

        nb=self.data[self.clf.predict(self.data[:,0:self.num_states+self.num_actions])==label]

        state_buffer=nb[:,0:self.num_states]
        action_buffer=nb[:,self.num_states:self.num_states+self.num_actions]
        reward_buffer=nb[:,self.num_states+self.num_actions:self.num_states+self.num_actions+1]
        next_state_buffer= nb[:,self.num_states+self.num_actions+1:]

        # print(len(state_buffer))

        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if len(state_buffer) < int(self.batch_size/2):
            states=state_buffer
            actions=action_buffer
            rewards=reward_buffer
            next_states=next_state_buffer
        else:
            select=random.sample(range(len(state_buffer)),int(self.batch_size/2))
            
            for i in select:
                states.append(state_buffer[i])
                actions.append(action_buffer[i])
                rewards.append(reward_buffer[i])
                next_states.append(next_state_buffer[i])
        self.pbL= [states, actions, rewards, next_states]
        # return states, actions, rewards, next_states
        # return pbL

    def get_batch(self):
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),self.batch_size)
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])
        return states, actions, rewards, next_states
    
    def get_batch1_2(self):
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size/2:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),int(self.batch_size/2))
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])

        states=np.concatenate((self.pbL[0],states),0)
        actions=np.concatenate((self.pbL[1],actions),0)
        rewards=np.concatenate((self.pbL[2],rewards),0)
        next_states=np.concatenate((self.pbL[3],next_states),0)

        return states, actions, rewards, next_states



# choose update based on proximity
class DDPGagent_m:   #modified ddpg for gmm process
    def __init__(self, n_states=1, n_actions=1,  actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, buffer_capacity=100000, batch_size=64, w_decay=0):
        # Params
        self.num_states = n_states
        self.num_actions = n_actions
        self.gamma = gamma
        self.tau = tau

        self.c_L=[]
        self.p_L=[]
        self.Qcat=[]

        # Networks
        self.actor = policy(self.num_states,self.num_actions)
        self.actor_target = policy(self.num_states,self.num_actions)
        self.critic = Qfun(self.num_states, self.num_actions)
        self.critic_target = Qfun(self.num_states, self.num_actions)

        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data)

        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data)
        
        # Training
        self.RB = Buffer2(self.num_states,self.num_actions,buffer_capacity)        
        self.critic_loss  = nn.MSELoss()
        self.actor_opt= torch.optim.Adam(self.actor.parameters(), lr=actor_learning_rate, weight_decay=w_decay)   #weight_decay
        self.critic_opt= torch.optim.Adam(self.critic.parameters(), lr=critic_learning_rate, weight_decay=w_decay)
    
        # self.gmm=mixer()

    def get_action(self, state):
        state = torch.tensor([state]).float()
        action = self.actor.forward(state)
        action = np.squeeze(action.detach().numpy())
        # action=np.ndarray.item(action)
        return action
    
    def update(self):
        
        states, actions, rewards, next_states = self.RB.get_batch()
        
            
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    
        # Critic loss        
        Qvals = self.critic.forward(states, actions)
        next_actions = self.actor_target.forward(next_states)
        next_Q = self.critic_target.forward(next_states, next_actions.detach())
        Qprime = rewards + self.gamma * next_Q
        critic_loss = self.critic_loss(Qvals, Qprime)
        self.c_L.append(critic_loss.detach().numpy())

        self.Qcat.append(Qvals.mean().item())

        # Actor loss
        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()
        self.p_L.append(policy_loss.detach().numpy())

        # update networks
        self.actor_opt.zero_grad()
        policy_loss.backward()
        self.actor_opt.step()

        self.critic_opt.zero_grad()
        critic_loss.backward() 
        self.critic_opt.step()

        # update target networks 
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))

    def update1_2(self):
        
        
        states, actions, rewards, next_states = self.RB.get_batch1_2()
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    
        # Critic loss        
        Qvals = self.critic.forward(states, actions)
        next_actions = self.actor_target.forward(next_states)
        next_Q = self.critic_target.forward(next_states, next_actions.detach())
        Qprime = rewards + self.gamma * next_Q
        critic_loss = self.critic_loss(Qvals, Qprime)
        self.c_L.append(critic_loss.detach().numpy())

        self.Qcat.append(Qvals.mean().item())

        # Actor loss
        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()
        self.p_L.append(policy_loss.detach().numpy())

        # update networks
        self.actor_opt.zero_grad()
        policy_loss.backward()
        self.actor_opt.step()

        self.critic_opt.zero_grad()
        critic_loss.backward() 
        self.critic_opt.step()

        # update target networks 
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))


def H_eR(prb,tempR):
    H=0
    R=0
    for grp,s in enumerate(prb):
            
            R=R+s*tempR[grp][1]
            if s <1e-6:
                H=H+0
            else:
                H=H-s*np.log2(s)
    return H,R


def H_eR_(prb,tempR):
    H=0
    R=0
    for grp,s in enumerate(prb):
            
            # R=R+s*tempR[grp][1]
            if s <1e-6:
                H=H+0
            else:
                H=H-s*np.log2(s)
    return H,R

class Buffer3:   #modifed buffer version 3
    def __init__(self, n_states=1, n_actions=1, buffer_capacity=100000, batch_size=64):
        # Number of "experiences" to store at max
        self.buffer_capacity = buffer_capacity
        # Num of tuples to train on.
        self.batch_size = batch_size

        # Its tells us num of times record() was called.
        self.buffer_counter = 0
        self.catalog=[]

        # Instead of list of tuples as the exp.replay concept go
        # We use different np.arrays for each tuple element
        self.state_buffer = []
        self.action_buffer = []
        self.reward_buffer = []
        self.next_state_buffer = []
        self.on = 0
        # self.state_buffer = np.zeros((self.buffer_capacity, n_states))
        # self.action_buffer = np.zeros((self.buffer_capacity, n_actions))
        # self.reward_buffer = np.zeros((self.buffer_capacity, 1))
        # self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))


        self.num_states=n_states
        self.num_actions=n_actions
        self.clf=[]
        self.exp=[]
        self.data=[]
        self.pbL=[]
        self.tempR=[]

        self.state_buffer_s = []
        self.action_buffer_s = []
        self.reward_buffer_s = []
        self.next_state_buffer_s = []

    # Takes (s,a,r,s') obervation tuple as input
    def record(self, obs_tuple):
        # Set index to zero if buffer_capacity is exceeded,
        # replacing old records
        index = self.buffer_counter % self.buffer_capacity
        
        if self.buffer_counter > 0 and index==0:
            self.on=1
        if self.on==1:
            self.state_buffer[index] = obs_tuple[0]
            self.action_buffer[index] = obs_tuple[1]
            self.reward_buffer[index] = obs_tuple[2]
            self.next_state_buffer[index] = obs_tuple[3]
        else:
            self.state_buffer.append(obs_tuple[0])
            self.action_buffer.append(obs_tuple[1])
            self.reward_buffer.append(obs_tuple[2])
            self.next_state_buffer.append(obs_tuple[3])

        self.buffer_counter += 1
            
    def generate(self, comp=7):

        del self.clf
        del self.exp
        
        sb=self.state_buffer
        ab=self.action_buffer
        rb=self.reward_buffer
        ssb=self.next_state_buffer

        # sb2=np.concatenate(sb).squeeze(1)
        # ab2=np.concatenate(ab).squeeze(1)
        # rb2=np.concatenate(rb).squeeze(1)
        # ssb2=np.concatenate(ssb).squeeze(1)

        # self.data=np.concatenate((sb,ab,rb),1)
        self.data=np.concatenate((sb,ab,rb,ssb),1)

        data2=self.data[:,0:Agent.num_states+Agent.num_actions]
        data3=self.data[:,0:Agent.num_states+Agent.num_actions+1]
        # self.clf = mixture.BayesianGaussianMixture(n_components=20, covariance_type="full", max_iter=1000 )
        self.clf = mixture.GaussianMixture(n_components=10, covariance_type="tied", max_iter=1000 )
        self.clf.fit(data3)


        # self.exp=mixture.BayesianGaussianMixture(n_components=20, covariance_type="full", max_iter=1000 )
        self.exp=mixture.GaussianMixture(n_components=10, covariance_type="full")
        self.exp.fit(data2)


        colors=["blue","orange","green","red","purple","black"]
        
        self.tempR=[]
        self.tempA=[]
        
        for i in range(self.exp.means_.shape[0]):
            # plt.plot()
            dsub = self.data[self.exp.predict(data2) == i]
            # plt.scatter(dsub[:, 0], dsub[:, 2], color=color, marker="x")
            self.tempR.append([i,np.mean(dsub[:,3]),np.std(dsub[:,3])])
            self.tempA.append([i,np.mean(dsub[:,2]),np.std(dsub[:,2])])
            # tempa.append([color,np.mean(dsub[:,2]),dsub[:,2]])
            # print(color)
            # print(np.mean(dsub[:,3]))
            # print(np.std(dsub[:,3]))
            # print('--------------------------------------------')

    def pred_exp(self,x,api):
        oo=21
        X=np.tile(x.T,(oo,1))
        A=np.expand_dims(np.linspace(-1,1,oo),1)
        prb=self.exp.predict_proba(np.concatenate((X,A),1))
        Entropy=[]
        ExpR=[]
        for i in prb:
            
            H, R_ =H_eR(i,self.tempR)
            # H=0
            # R_=0
            # for grp,s in enumerate(i):
                
            #     R_=R_+s*self.tempR[grp][1]
            #     if s <1e-6:
            #         H=H+0
            #     else:
            #         H=H-s*np.log2(s)
            Entropy.append(H)
            ExpR.append(R_)
        Rmin=max(ExpR)
        Index=np.where(max(ExpR)==ExpR)[0][0]
        Hrmin=Entropy[Index]



        prbpi=self.exp.predict_proba(np.concatenate((x.T,api),1))
        # Hpi=0
        # Rpi=0
        # for grp,s in enumerate(prbpi[0]):
            
        #     Rpi=Rpi+s*self.tempR[grp][1]
        #     if s <1e-6:
        #         Hpi=Hpi+0
        #     else:
        #         Hpi=Hpi-s*np.log2(s)
        Hpi, Rpi=H_eR(prbpi[0],self.tempR)

        IN_min=np.concatenate((x.T,np.expand_dims(A[Index],1),np.array([[Rmin]])),1)
        IN_pi=np.concatenate((x.T,api,np.array([[Rpi]])),1)

        out_min=self.clf.predict(IN_min)
        prb_min=self.clf.predict_proba(IN_min)
        out_pi=self.clf.predict(IN_pi)
        prb_pi=self.clf.predict_proba(IN_pi)
        
        Rmin_clf=np.dot(prb_min,self.clf.means_[:,3:])
        Rpi_clf=np.dot(prb_pi,self.clf.means_[:,3:])

        H__m=H_eR_(prb_min[0],self.tempR)[0]
        H__pi=H_eR_(prb_pi[0],self.tempR)[0]

        # Index2=np.where(max(Entropy)==Entropy)[0][0]
        # R_H1max=ExpR[Index2]






        fsin=np.concatenate((X,A,np.array([ExpR]).T),1)

        fullprb=self.clf.predict_proba(fsin)
        fl=self.clf.predict(fsin)
        ind3=np.where(max(self.clf.means_[:,3:])==self.clf.means_[:,3:])[0][0]
        ind4=np.where(fl==ind3)[0]

        Entropy2=[]
        for i in fullprb:
            
            H, R_ =H_eR_(i,self.tempR)
            Entropy2.append(H)

        Index2=np.where(max(Entropy2)==Entropy2)[0][0]
        R_H1max=ExpR[Index2]


        # subcheck=False
        # Hsub=[]
        # H_check=[]
        # for z in fullprb:
        #     H_=H_eR_(z,self.tempR)[0]
        #     H_check.append(H_)
        # if len(ind4)>0:
        #     subcheck=True
        #     for w in ind4:
        #         Hsub.append([w, H_check[w]])
        #     Hsub=np.array(Hsub)
        #     maxgrp=max(Hsub[:,1])
        #     Hind=Hsub[np.where(max(Hsub[:,1])==maxgrp)]


            # print(Hind)
        # tds=[]
        # for i in range(self.clf.means_.shape[0]):
            
        #     dsub =fsin[self.clf.predict(fsin) == i]
        #     tds.append(dsub)
        
        

        # subcheck=False
        # if len(tds[ind3])>0:
        #     subprb=self.clf.predict_proba(tds[ind3])
        #     H_check=H_eR_(subprb[0],self.tempR)[0]
        #     Hind=np.where(max(H_check)==H_check)
        #     subcheck=True

        if out_min==out_pi:
            RminComp=Rmin
            RpiComp=Rpi

            # RminComp=Rmin/(H__m+1e-2)
            # RpiComp=Rpi/(H__pi+1e-2)


            if R_H1max>(np.mean(Agent.RB.data[:,3])):
                action=np.expand_dims(A[Index2],1)+np.random.normal()*.03
                self.catalog.append(4)


            # if RminComp>RpiComp:
            #     action=np.expand_dims(A[Index],1)+np.random.normal()*.03
            #     self.catalog.append(0)
            # else:
            #     action=api+np.random.normal()*.03
            #     self.catalog.append(1)

            # action=np.expand_dims(A[int(Hind[0,0].item())],1) +np.random.normal()*.03
            # self.catalog.append(4)

            # action=api
            # self.catalog.append(0)

            elif H__m>H__pi:
                action=np.expand_dims(A[Index],1)+np.random.normal()*.03
                self.catalog.append(2)

            else:
                action=api+np.random.normal()*.03
                self.catalog.append(3)



        # elif subcheck:
        #     action=np.expand_dims(A[int(Hind[0,0].item())],1) +np.random.normal()*.03
        #     self.catalog.append(4)

        else:
            RminComp=Rmin
            RpiComp=Rpi

            if RminComp>RpiComp:
                action=np.expand_dims(A[Index],1)+np.random.normal()*.03
                self.catalog.append(0)
            else:
                action=api+np.random.normal()*.03
                self.catalog.append(1)
            
            # if Hrmin>Hpi:
            #     action=np.expand_dims(A[Index],1)+np.random.normal()*.03
            #     self.catalog.append(2)

            # else:
            #     action=api+np.random.normal()*.03
            #     self.catalog.append(3)

            # action=api+np.random.normal()*.25
            # action=np.expand_dims(A[Index],1)+np.random.normal()*.03
            # self.catalog.append(4)




        # thresh=0.9

        # gg=[]
        # grp=[]
        # pick=[]
        # pick2=[]
        # pick3=[]
        # a=np.array(self.tempR)
        # a=a[a[:, 1].argsort()]
        # if np.where(prb[:,int(a[-1,0])]>0.9)[0].size>0:
        #     for i in np.where(prb[:,int(a[-1,0])]>0.9)[0]:
        #         pick.append(i)
        #         # grp.append(prb[i,:])

        # if np.where((prb<thresh) & (prb>0.1))[0].size>0:
        #     # nva=False

        #     for i in np.where((prb<thresh) & (prb>0.1))[0]:
        #        pick.append(i)


               
        # pick=list(set(pick))
        # for i in pick:
        #     grp.append(prb[i,:])

        # if len(pick)>0:
        #     nva=False
        #     indx=random.choice(pick)
        #     action=A[indx] +np.random.normal()*.03
        #     print(prb[indx,:])
        # else:
        #     nva=True
        #     action=[]

        
        # return nva, action
        return action
        

    def predict_set(self,x,a, comp=6):
        # self.generate(comp)

        # label=self.clf.predict(np.concatenate((x.T,a),1))
        # prb=self.clf.predict_proba(np.concatenate((x.T,a),1))

        means=self.clf.means_
        sel=max(means[:,3])

        nb=self.data[self.clf.predict(self.data[:,0:Agent.num_states+Agent.num_actions+1])==np.where(means==sel)[0].item()]

        state_buffer=nb[:,0:self.num_states]
        action_buffer=nb[:,self.num_states:self.num_states+self.num_actions]
        reward_buffer=nb[:,self.num_states+self.num_actions:self.num_states+self.num_actions+1]
        next_state_buffer= nb[:,self.num_states+self.num_actions+1:]

        # print(len(state_buffer))

        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if len(state_buffer) < int(self.batch_size/2):
            states=state_buffer
            actions=action_buffer
            rewards=reward_buffer
            next_states=next_state_buffer
        else:
            select=random.sample(range(len(state_buffer)),int(self.batch_size/2))
            
            for i in select:
                states.append(state_buffer[i])
                actions.append(action_buffer[i])
                rewards.append(reward_buffer[i])
                next_states.append(next_state_buffer[i])
        self.pbL= [states, actions, rewards, next_states]
        # return states, actions, rewards, next_states
        # return pbL

    def get_batch(self):
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),self.batch_size)
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])
        return states, actions, rewards, next_states
    
    def get_batch1_2(self):
        states=[]
        actions=[]
        rewards=[]
        next_states=[]
        
        
        if self.buffer_counter < self.batch_size/2:
            states=self.state_buffer
            actions=self.action_buffer
            rewards=self.reward_buffer
            next_states=self.next_state_buffer
        else:
            select=random.sample(range(len(self.state_buffer)),int(self.batch_size/2))
            
            for i in select:
                states.append(self.state_buffer[i])
                actions.append(self.action_buffer[i])
                rewards.append(self.reward_buffer[i])
                next_states.append(self.next_state_buffer[i])

        states=np.concatenate((self.pbL[0],states),0)
        actions=np.concatenate((self.pbL[1],actions),0)
        rewards=np.concatenate((self.pbL[2],rewards),0)
        next_states=np.concatenate((self.pbL[3],next_states),0)

        return states, actions, rewards, next_states



# Choose Update for reard
class DDPGagent_m2:   #ddpg version 3 modified for gmm update and exp
    def __init__(self, n_states=1, n_actions=1,  actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, buffer_capacity=100000, batch_size=64, w_decay=0):
        # Params
        self.num_states = n_states
        self.num_actions = n_actions
        self.gamma = gamma
        self.tau = tau

        self.c_L=[]
        self.p_L=[]
        self.Qcat=[]

        # Networks
        self.actor = policy(self.num_states,self.num_actions)
        self.actor_target = policy(self.num_states,self.num_actions)
        self.critic = Qfun(self.num_states, self.num_actions)
        self.critic_target = Qfun(self.num_states, self.num_actions)

        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data)

        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data)
        
        # Training
        self.RB = Buffer3(self.num_states,self.num_actions,buffer_capacity)        
        self.critic_loss  = nn.MSELoss()
        self.actor_opt= torch.optim.Adam(self.actor.parameters(), lr=actor_learning_rate, weight_decay=w_decay)   #weight_decay
        self.critic_opt= torch.optim.Adam(self.critic.parameters(), lr=critic_learning_rate, weight_decay=w_decay)
    
        # self.gmm=mixer()

    def get_action(self, state):
        state = torch.tensor([state]).float()
        action = self.actor.forward(state)
        action = np.squeeze(action.detach().numpy())
        # action=np.ndarray.item(action)
        return action
    
    def get_action1_2(self, state_):
        x = np.array([state_]).T
        state = torch.tensor([state_]).float()
        action = self.actor.forward(state)
        api = action.detach().numpy()

        asub=self.RB.pred_exp(x,api)
        # nva, asub=self.RB.pred_exp(x,api)

        # if nva:
        #     state = torch.tensor([state_]).float()
        #     action = self.actor.forward(state)
        #     action = np.squeeze(action.detach().numpy())
        #     # action=np.ndarray.item(action)
        # else:
        #     action=np.array(asub)
        nva=False
        action=asub
        return action, nva

    def update(self):
        
        states, actions, rewards, next_states = self.RB.get_batch()
        
            
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    
        # Critic loss        
        Qvals = self.critic.forward(states, actions)
        next_actions = self.actor_target.forward(next_states)
        next_Q = self.critic_target.forward(next_states, next_actions.detach())
        Qprime = rewards + self.gamma * next_Q
        critic_loss = self.critic_loss(Qvals, Qprime)
        self.c_L.append(critic_loss.detach().numpy())

        self.Qcat.append(Qvals.mean().item())

        # Actor loss
        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()
        self.p_L.append(policy_loss.detach().numpy())

        # update networks
        self.actor_opt.zero_grad()
        policy_loss.backward()
        self.actor_opt.step()

        self.critic_opt.zero_grad()
        critic_loss.backward() 
        self.critic_opt.step()

        # update target networks 
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))

    def update1_2(self):
        
        
        states, actions, rewards, next_states = self.RB.get_batch1_2()
        states = torch.FloatTensor([states]).transpose(0,1).float()
        actions = torch.FloatTensor([actions]).transpose(0,1).float()
        rewards = torch.FloatTensor([rewards]).transpose(0,1).float()
        next_states = torch.FloatTensor([next_states]).transpose(0,1).float()
    
        # Critic loss        
        Qvals = self.critic.forward(states, actions)
        next_actions = self.actor_target.forward(next_states)
        next_Q = self.critic_target.forward(next_states, next_actions.detach())
        Qprime = rewards + self.gamma * next_Q
        critic_loss = self.critic_loss(Qvals, Qprime)
        self.c_L.append(critic_loss.detach().numpy())

        self.Qcat.append(Qvals.mean().item())

        # Actor loss
        policy_loss = -self.critic.forward(states, self.actor.forward(states)).mean()
        self.p_L.append(policy_loss.detach().numpy())

        # update networks
        self.actor_opt.zero_grad()
        policy_loss.backward()
        self.actor_opt.step()

        self.critic_opt.zero_grad()
        critic_loss.backward() 
        self.critic_opt.step()

        # update target networks 
        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))
       
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(param.data * self.tau + target_param.data * (1.0 - self.tau))





# %% SCR unit
class SCR:   #selective catalytric reduction unit (linear and nonlinear state space model)
    def __init__(self,du_sc=None,u_sc=None,y_sc=None):
        self.h = 0.1
        self.A = np.array([[0.9464,0.0056],
                          [0.0043,0.1207]])
        self.B = np.array([[-0.0100],
                          [0.0040]])
        
        ####################################################################
        self.C = np.array([[1,0],[0,1]])
        # self.C = np.array([[1,0]])
        ####################################################################

        self.f1 = 0.95
        self.f2 = 1.05
        self.Aoffset = np.array([[self.f1*0.9464,self.f2*0.0056],
                          [self.f2*0.0043,self.f1*0.1207]])
        self.Boffset = np.array([[self.f2*-0.0100],
                          [self.f2*0.0040]])
        self.Coffset = np.array([1,0])

        self.Hy = np.array([[0.9985, 0.4769],[-0.0105,0.4804]])
        self.Hu = np.array([[-0.0030, -4.9521e-6],[0.0023, 1.6833e-6]])
        self.ysp = 0

        ####################################################################
        # self.Q=np.array([[1, 0], [0, .01]])
        self.Q=np.array([[100, 0], [0, 1]])
        # self.Q=np.eye(self.C.shape[0])
        ####################################################################

        self.rw=0.0001
        self.uprev=0

        self.du_sc=du_sc
        self.u_sc=u_sc
        self.y_sc=y_sc


    def model(self,x,u):
        xplus = self.A@x + self.B@u
        yplus = self.C@xplus
        return xplus, yplus

    def modeloffset(self,x,u):
        xplus = self.Aoffset@x + self.Boffset@u
        yplus = self.Coffset@xplus
        return xplus, yplus
    
    def nlmodel(self,x,u):
        u = np.array([[u.item()],[u.item()**2]])
        xplus = self.Hy@x + self.Hu@u
        yplus = self.C@xplus
        return xplus,yplus

    def reward(self,x,ysp,u,uprev=None):
        if uprev is not None:
            du=u-uprev
        else:
            du=u-self.uprev
        # Q = np.array([[100, 0],
        #               [0, 1]])
        # ymod=self.C@x-ysp
        # du=u-self.uprev
        #R = -np.matmul(np.transpose(x),x)-np.matmul(np.transpose(u),u)
        # R = -(x.T@Q@x)-0.0001*u.T@u
        # R = -(x[0])**2


        if self.y_sc is not None:
            yn=self.C@x
            for i in range(ss.C.shape[0]):
            # for i in range(len(self.y_sc)):
                yn[i,0]=self.y_sc[i].norm(yn[i,0])
                ysp=self.y_sc[i].norm(ysp)
            ymod=yn-ysp
        else:
            ymod=self.C@x-ysp
        if self.du_sc is not None:
            for i in range(len(self.du_sc)):
                du[i,0]=self.du_sc[i].norm(du[i,0])


        R=-(np.matmul(np.matmul(ymod.T,self.Q),ymod))-self.rw*np.matmul(du,du)
        return R
    
    def Gcalc(self,xtraj,utraj,Np,gamma):
            Gcat=[0]
            rcat=[0]
            G=0
            for i in range(1,len(xtraj[0])-1):
            # for i in range(Np-1):


                
            
                R=self.reward(np.array([[  xtraj[0][i], xtraj[1][i] ]]).T, self.ysp,   np.array([[  utraj[0][i] ]]),np.array([[  utraj[0][i-1] ]])  )

                # if i==0:
                #     R=self.reward(np.array([[  xtraj[0][i], xtraj[1][i] ]]).T, self.ysp,   np.array([[  utraj[0][i] ]]))
                # else:
                #     R=self.reward(np.array([[  xtraj[0][i], xtraj[1][i] ]]).T, self.ysp,   np.array([[  utraj[0][i] ]]),np.array([[  utraj[0][i-1] ]])  )
                rcat.append(R.squeeze().tolist())
                G=G+R.squeeze().tolist()*gamma**(i+1)
                Gcat.append(G)
            return Gcat,rcat


    def stepmodel(self,x,u):
        xp, yp = self.model(x,u)
        dx = xp - x
        
        R = self.reward(xp,self.ysp,u)
        if (R>-1e-6) or abs(xp[0,0])>5 or abs(xp[1,0])>5:
          isdone = 1
        else:
          isdone = 0
        #print(isdone)
        self.uprev=u
        return xp,yp, R, isdone

    def stepmodeloffset(self,x,u):
        #xp, yp = self.modeloffset(x,u)
        xp, yp = self.nlmodel(x,u)
        dx = xp - x
        
        R = self.reward(xp,self.ysp,u)
        if (R>-1e-6) or abs(xp[0,0])>5 or abs(xp[1,0])>5:
          isdone = 1
        else:
          isdone = 0
        #print(isdone)
        self.uprev=u
        return xp,yp, R, isdone

    def reset(self,maxk):
      s = np.zeros((2,1))
      y = np.zeros((1,))
      posneg = np.array([-1,1])
      s[0,0] = 4*posneg[np.random.randint(2)]*np.random.rand()
      s[1,0] = 4*posneg[np.random.randint(2)]*np.random.rand()
      dmag = 0.5*posneg[np.random.randint(2)]*np.random.rand()
      return s, y, dmag


#  %% Path



# print('G LOCAL 13')
# pbse=(r'G:\My Drive\Python Scripts\conf_set\glocal13')
# path_c1_s=(pbse+'\critic1')
# path_ct1_s=(pbse+'\critic_target1')
# path_a_s=(pbse+r'\actor')
# path_at_s=(pbse+r'\actor_target')


# %% spec opt
# RB_=t1.RB.buffer
# cat_=t1.cat.buffer






###################################################################
###################################################################
###################################################################
import numpy as np
from matplotlib import pyplot as plt
import pickle

def loss_plot(pfile_L):
    with open(pfile_L, "rb") as pkl_rb_obj:
        Loss = pickle.load(pkl_rb_obj)  
    # Loss=np.array(t1.rbfL)

# pfile_L=pbse + '\Loss.pkl'

#         Loss=[Agent.p_L,Agent.c_L,rcat]

    loss1=Loss[0]
    loss2=Loss[1]
    rcat=Loss[2]
    
    
    Ml1=mov_mean(loss1,65*5)
    Ml2=mov_mean(loss2,65*5)
    
    rvg=mov_med(rcat,65*3)

    plt.figure()
    plt.plot(loss1)
    plt.plot(Ml1)
    plt.title('Policy')
    plt.grid()
    plt.figure()
    plt.plot(loss2)
    plt.plot(Ml2)
    plt.title('Critic')
    plt.grid()


    plt.figure()
    plt.plot(loss1[int(len(loss1)/2):])
    plt.plot(Ml1[int(len(loss1)/2):])
    plt.title('Policy 1/2')
    plt.grid()
    plt.figure()
    plt.plot(loss2[int(len(loss2)/2):])
    plt.plot(Ml2[int(len(loss2)/2):])
    plt.title('Critic 1/2')
    plt.grid()


    plt.figure()
    plt.plot(loss1[int(len(loss1)/2):])
    plt.plot(Ml1[int(len(loss1)/2):])
    plt.title('Policy 1/2')
    plt.grid()
    plt.ylim([-0.01, max(Ml1[int(len(loss1)/2):])*1.1])
    
    plt.figure()
    plt.plot(loss2[int(len(loss2)/2):])
    plt.plot(Ml2[int(len(loss2)/2):])
    plt.title('Critic 1/2')
    plt.grid()
    plt.ylim([-0.01, max(Ml2[int(len(loss2)/2):])*1.1])
    plt.figure()


    

    plt.figure()
    plt.plot(rcat)
    plt.plot(rvg)

# pfile_L=pbse + '\Loss.pkl'
# loss_plot(pfile_L)
# %% Initialize


#Double Int   (initalization  for double integrator model)
# dulb=[-5]
# duub=[5]
# ulb=[-10]
# uub=[10]
# ylb=[-20]
# yub=[20]
# du_sc=create_scales_tanh(dulb,duub)
# u_sc=create_scales_tanh(ulb,uub)
# y_sc=create_scales_tanh(ylb,yub)
# ss=double_int(du_sc,u_sc,y_sc)
# di_status=True    #using Double integrator model
# scr_status=False    #using SCR model

# # ss.C=np.eye(2)
# # ss.Q=np.eye(2)


#SCR   (initalization  for double integrator model)
dulb=[-100]
duub=[100]
ulb=[-400]
uub=[400]
ylb=[-10, -3]
yub=[10, 3]
du_sc=create_scales_tanh(dulb,duub)
u_sc=create_scales_tanh(ulb,uub)
y_sc=create_scales_tanh(ylb,yub)
ss=SCR(du_sc,u_sc,y_sc)
di_status=False    #using Double integrator model
scr_status=True    #using SCR model



sp=0


wd=0
bs=128
n_states=2
n_actions=1
Agent=DDPGagent_m2(n_states,n_actions, 1e-4, 1e-3, 0.99, 0.01, 100000, bs, wd)   #reward rank

# (n_states=1, n_actions=1,  actor_learning_rate=1e-4, critic_learning_rate=1e-3, gamma=0.99, tau=1e-2, buffer_capacity=100000, batch_size=64, w_decay=0):)




# nam_con="ep_500_01132025_special_train_standard"
# pfile=(r"G:\My Drive\Python Scripts\conf_set\data\buffile_"+nam_con+".pkl")
# with open(pfile, "rb") as pkl_rb_obj:
#         buffile = pickle.load(pkl_rb_obj) 

# Agent.RB.state_buffer=buffile[:,0:2].tolist()
# Agent.RB.action_buffer=buffile[:,2:3].tolist()
# Agent.RB.reward_buffer=buffile[:,3:4].tolist()
# Agent.RB.next_state_buffer=buffile[:,4:].tolist()
# Agent.RB.buffer_counter=len(buffile[:,0])

# %% Settings

special_train=False
special_exp=False

#Train
epmax=500
tfinal=30
# noise=0.01 ##########################
noise=0.25 ##########################
# noise=0.32 ##########################
static_nn=False
save_nn=False   ###############################################
dist_select=False
decay_dist_select=False
decay_B=False
switch_B=False
save_loss=False   ###############################################



#Test
# epmax=10
# tfinal=30
# noise=0
# static_nn=True    #not training NN model or updating OMLT
# save_nn=False      #save NN
# dist_select=True   #use select disturbances
# decay_dist_select=False
# decay_B=False
# switch_B=False
# save_loss=False
# special_train=False
# special_exp=False

# decay loop
# epmax=100
# tfinal=60
# noise=0
# static_nn=True #############################
# save_nn=False
# dist_select=False
# decay_dist_select=True
# decay_B=False
# switch_B=False
# save_loss=False
# t1.iter=10

#exp
# epmax=1
# tfinal=10
# noise=0.0 ##########################
# # noise=0.15 ##########################
# static_nn=True
# save_nn=False   ###############################################
# dist_select=False
# decay_dist_select=True
# decay_B=False
# switch_B=False
# save_loss=False
# t1.iter=10


offset=True       #use offset SCR model vs MPC mode (differing models)
rl_status=True      #use RL over MPC
# nlmodel=False   #use nl model constraints in MPC
save_data=False  #record data for trial

temp_mod=False

Hu0=ss.Hu
Hy0=ss.Hy

DmatY=np.array([[(1), (.41)],[(.95),(0.98)]])
DmatU=np.array([[(0.8), (0.9)],[(2.51),(2)]])

# ss.Hy=np.multiply(Hy0,DmatY)
# ss.Hu=np.multiply(Hu0,DmatU)


################################################################
# ss.C=np.array([[1,0]])
# ss.Q=np.array([[1]])

OUA=OUActionNoise(np.array(0),sigma_max=noise, sigma_min=0.0, theta=0.15, dt=1, decay_period=epmax*tfinal)
# %%

# path_c1_s=(pbse+'\critic1')
# path_ct1_s=(pbse+'\critic_target1')
# path_a_s=(pbse+r'\actor')
# path_at_s=(pbse+r'\actor_target')

# Agent.actor= torch.load(path_a_s)
# Agent.actor_target= torch.load(path_at_s)
# Agent.critic= torch.load(path_c1_s)
# Agent.critic_target= torch.load(path_ct1_s)


# %%   Sample disturbances for testing

dist_scr=[np.array([[0.62053369],
        [1.22600576]]),
 np.array([[-3.52617473],
        [-1.24147056]]),
 np.array([[4.36503407],
        [1.33052744]]),
 np.array([[ 0.12886711],
        [-0.10233823]]),
 np.array([[-0.44564937],
        [-1.49241653]]),
 np.array([[-3.08370142],
        [ 0.12104975]]),
 np.array([[-6.46658544],
        [-0.54416887]]),
 np.array([[ 4.63572076],
        [-1.94529455]]),
 np.array([[ 4.24823588],
        [-0.28993123]]),
 np.array([[-2.76261956],
        [ 1.29750454]])]

decay_dist=[np.array([[ 3.50077325],
                    [-2.33552661]]),
            np.array([[ 3.33137922],
                    [-4.64623176]]),
            np.array([[-2.98269842],
                    [ 6.30570546]]),
            np.array([[5.88421606],
                    [3.7294569 ]]),
            np.array([[-4.04633107],
                    [ 6.41195325]]),
            np.array([[0.26864626],
                    [4.06348675]]),
            np.array([[-5.17764778],
                    [-2.68035882]]),
            np.array([[-2.72876116],
                    [ 1.74856342]]),
            np.array([[-4.64235626],
                    [ 0.28398791]]),
            np.array([[ 1.83219793],
                    [-4.10379263]]),
            np.array([[-1.86921947],
                    [-5.25677072]]),
            np.array([[ 5.37234666],
                    [-1.50578377]]),
            np.array([[-5.70520458],
                    [-1.95928178]]),
            np.array([[0.61637251],
                    [2.29471977]]),
            np.array([[-1.95961601],
                    [-6.09868303]]),
            np.array([[-2.77083759],
                    [ 2.64723714]]),
            np.array([[-0.38399841],
                    [ 4.66617444]]),
            np.array([[5.68586023],
                    [6.08430261]]),
            np.array([[-1.54519979],
                    [ 5.12864578]]),
            np.array([[-6.0796418 ],
                    [ 0.35093431]]),
            np.array([[0.93717445],
                    [0.63810375]]),
            np.array([[ 5.65503803],
                    [-1.58104562]]),
            np.array([[6.21042536],
                    [0.49830966]]),
            np.array([[ 4.00344218],
                    [-0.45787868]]),
            np.array([[1.57142141],
                    [1.54029018]]),
            np.array([[-1.1383341 ],
                    [ 4.46784676]]),
            np.array([[1.16433632],
                    [4.51518904]]),
            np.array([[-6.43805268],
                    [ 0.12271865]]),
            np.array([[-0.29191634],
                    [-1.72292959]]),
            np.array([[-0.14522268],
                    [ 4.5894942 ]]),
            np.array([[3.82991386],
                    [2.9612656 ]]),
            np.array([[ 5.61617054],
                    [-1.32354662]]),
            np.array([[-1.01846038],
                    [ 0.40372778]]),
            np.array([[1.61659645],
                    [3.18366913]]),
            np.array([[-3.41044371],
                    [-0.44912873]]),
            np.array([[ 1.61939032],
                    [-3.93888612]]),
            np.array([[ 3.65715408],
                    [-1.24682799]]),
            np.array([[-5.49071254],
                    [-4.80396837]]),
            np.array([[ 1.63736536],
                    [-4.86428674]]),
            np.array([[ 5.07069418],
                    [-1.13849344]]),
            np.array([[ 0.23021372],
                    [-4.53911576]]),
            np.array([[4.79227135],
                    [5.21727131]]),
            np.array([[-0.67765521],
                    [-0.00765028]]),
            np.array([[6.03709894],
                    [5.41292823]]),
            np.array([[-5.77627954],
                    [ 3.12717395]]),
            np.array([[4.11147433],
                    [2.83617468]]),
            np.array([[0.17649724],
                    [0.35610993]]),
            np.array([[4.27102832],
                    [3.14270392]]),
            np.array([[-4.93805068],
                    [-5.66214015]]),
            np.array([[0.32477454],
                    [0.83562004]]),
            np.array([[5.70847398],
                    [0.80842556]]),
            np.array([[0.80875327],
                    [1.69211904]]),
            np.array([[-1.15172389],
                    [-6.19651167]]),
            np.array([[-1.99734725],
                    [ 6.47514326]]),
            np.array([[-0.57288647],
                    [-4.00863395]]),
            np.array([[-5.9463352 ],
                    [ 3.71723876]]),
            np.array([[-2.44166643],
                    [-1.6859975 ]]),
            np.array([[-4.28051009],
                    [ 1.64998245]]),
            np.array([[4.07103332],
                    [3.48691964]]),
            np.array([[-4.99208874],
                    [-4.03230518]]),
            np.array([[ 2.62601826],
                    [-4.51050717]]),
            np.array([[-4.87286682],
                    [ 0.98898742]]),
            np.array([[2.60091335],
                    [6.00409839]]),
            np.array([[5.71022643],
                    [6.02923244]]),
            np.array([[6.16572113],
                    [3.73015519]]),
            np.array([[1.73768291],
                    [3.53956175]]),
            np.array([[-1.82388301],
                    [ 5.49140591]]),
            np.array([[ 3.32473733],
                    [-1.5317327 ]]),
            np.array([[ 2.34890871],
                    [-4.60431061]]),
            np.array([[ 5.75809124],
                    [-5.58417785]]),
            np.array([[3.31016934],
                    [4.8805454 ]]),
            np.array([[3.25726186],
                    [1.52779955]]),
            np.array([[-6.06880423],
                    [-5.46957073]]),
            np.array([[3.34509959],
                    [3.35935444]]),
            np.array([[ 2.85318502],
                    [-3.37963432]]),
            np.array([[ 6.36794617],
                    [-2.14806702]]),
            np.array([[-0.58882523],
                    [ 5.3817134 ]]),
            np.array([[-0.84954327],
                    [ 1.70747192]]),
            np.array([[-0.35372706],
                    [-5.96011221]]),
            np.array([[ 4.17702116],
                    [-1.5707476 ]]),
            np.array([[-2.15342932],
                    [ 3.02623577]]),
            np.array([[-2.45838955],
                    [ 6.24883974]]),
            np.array([[-0.51752489],
                    [-1.68630143]]),
            np.array([[-0.9465436 ],
                    [ 1.64099264]]),
            np.array([[-3.89755643],
                    [ 5.66339133]]),
            np.array([[ 4.8326743 ],
                    [-2.95856404]]),
            np.array([[ 1.8225697 ],
                    [-0.22613828]]),
            np.array([[0.59835606],
                    [4.55386007]]),
            np.array([[-5.96175076],
                    [-0.35394058]]),
            np.array([[-0.64346648],
                    [-4.88498333]]),
            np.array([[-5.84805203],
                    [-0.72019513]]),
            np.array([[3.96368814],
                    [0.61813875]]),
            np.array([[-2.12912745],
                    [ 3.55285253]]),
            np.array([[-6.05029572],
                    [ 1.78347625]]),
            np.array([[ 1.47482031],
                    [-4.21020742]]),
            np.array([[3.52032813],
                    [5.25475701]]),
            np.array([[-1.7384985 ],
                    [-3.45929679]]),
            np.array([[-3.29685479],
                    [ 5.69439143]]),
            np.array([[ 2.18697549],
                    [-4.05695451]]),
            np.array([[-0.6036533 ],
                    [-5.27697924]])]



# %% Loop



u=np.array([[0]])
unew=np.zeros((1,1))


breaktimes_act=[]
breaktimes_up=[]
breaktimes_complete=[]
breaktimes_BKC=[]
terminate_cond=[]

skip_update_on_fail=False
terminate_frac=(2/3)

JGcat=[]
rcat=[]

rvg_cat=[]

utraj_cat=[]
xtraj_cat=[]
xcat=[]
ycat=[]
ucat=[u.squeeze().tolist()]
Jcat=[]
ncat=[]
ucat_n=[]
ltR_=time.time()
lts=time.time()
jvb=False
for ep in range(epmax):
    if switch_B:
        if ep==int(epmax/2):
            ss.Hu=Hu0*0.1
    rvg=[]
    # if ep%fig_mod==0 and ep!=0:
    #     # try:
    #     plt.figure()
    #     plt.plot(np.array(ycat)[-31*20:-1])


    BKC=0

    LDT=(time.time()-lts)/60
    print('LOOP TIME:',LDT,'--------------------------------')
    lts=time.time()
    stt=time.time()
    
    if decay_dist_select:
        x=decay_dist[ep]
    else:
        if di_status and dist_select:
            x=dist[ep]
        elif scr_status and dist_select:
            x=dist_scr[ep]
        elif scr_status and not dist_select:
            x=np.array([[0.66*rand.choice([-1,1])*yub[0]*np.random.rand(),0.66*rand.choice([-1,1])*yub[1]*np.random.rand()]]).T
            # x=np.array([[0.3*rand.choice([-1,1])*yub[0]*np.random.rand(),0.3*rand.choice([-1,1])*yub[1]*np.random.rand()]]).T

        else:
            x=np.array([[rand.choice([-1,1])*yub[0]*np.random.rand(),rand.choice([-1,1])*yub[0]*np.random.rand()]]).T
    
    y=np.matmul(ss.C,x)
    ycat.append(y.squeeze().tolist())

    for t in range(tfinal):
        # stt=time.time()
        print((ep+t/tfinal)/epmax*100,'  TIME:',time.time()-stt)
        stt=time.time()
        xcat.append(x.squeeze().tolist())
        
        if special_train or special_exp:
            # if ep>1 and t==0:
            #     Agent.RB.generate()
            if ep==0 and t==29:
                Agent.RB.generate()
                jvb=True
            elif ep>1 and t==0:
                Agent.RB.generate()
                jvb=True

        if rl_status:
            if special_exp and jvb:
            # if special_exp and ep>1:
                unew, nva=Agent.get_action1_2( [y_sc[0].norm(y[0].item()),y_sc[0].norm(y[1].item())] )
                # ns_=False
            else:
                unew=Agent.get_action( [y_sc[0].norm(y[0].item()),y_sc[0].norm(y[1].item())] )
                nva=True

            
        else:
           
            print('-------------------------------check----------------------------------')
        
        ucat_n.append(unew)
        # xtraj_cat.append(xtraj)
        # utraj_cat.append(unew)
        # JGcat.append([J,G])

        if ep==(epmax-1) or noise==0 or not nva:
            u=np.array([[u_sc[0].act(unew)]]) #needs fixed for other inputs
            # u=np.array([[unew[0][0]]]).T #needs fixed for other inputs
        else:
            nois=OUA(ep*tfinal+t)
            ncat.append(np.clip(nois,-1,1))
            u=np.clip(np.array([[u_sc[0].act(unew)]])+u_sc[0].act(np.clip(nois,-1,1)),ulb,uub)
            # u=np.clip(np.array([[unew[0][0]]]).T+u_sc[0].act(np.clip(nois,-1,1)),ulb,uub)
            # u=np.clip(np.array([[unew[0][0]]]).T+du_sc[0].act(np.random.randn()*noise*(1-ep/epmax)),ulb,uub)
        ucat.append(u.squeeze().tolist())

        if scr_status and offset:
            xp,yp, R, isdone=ss.stepmodeloffset(x,u)    
        else:
            xp,yp, R, isdone=ss.stepmodel(x,u)

        rcat.append(R.squeeze().item())
        rvg.append(R.squeeze().item())
        ycat.append(yp.squeeze().tolist())

        if BKC>(terminate_frac)*tfinal:
            breaktimes_BKC.append([ep,t])

        if any(xp>(yub[0]*1.1)) or any(xp<(ylb[0]*1.1)) or BKC>(terminate_frac)*tfinal:
            u=np.array([[0]])
            terminate_cond.append([ep,t])
            break

        
            # t1.update(x,y,u,R,ytraj2,unew2,xtraj2,ss)
            # t1.update_rbf(x,y,u,R,ytraj2,unew2,xtraj2,ss)
            # print('update')
            
            
        
        if not static_nn:

            p=([y_sc[0].norm(y[0].item()),y_sc[1].norm(y[1].item())],[u_sc[0].norm(u.item())],[R.item()],[y_sc[0].norm(yp[0].item()),y_sc[1].norm(yp[1].item())])
            Agent.RB.record(p)

            if special_train:

                

                if ep>1:
                    Agent.RB.predict_set(np.array([[y_sc[0].norm(y[0].item())],[y_sc[1].norm(y[1].item())]]),np.expand_dims(unew,axis=(0,1)))
                    Agent.update1_2()
                else:
                    Agent.update()
            else:
                Agent.update()
                

        if save_nn:
            torch.save(Agent.critic,path_c1_s)
            torch.save(Agent.critic_target,path_ct1_s)
            torch.save(Agent.actor,path_a_s)
            torch.save(Agent.actor_target,path_at_s)
            
            # path_c1_s=(pbse+'\critic1')
            # path_ct1_s=(pbse+'\critic_target1')
            # path_a_s=(pbse+'\actor')
            # path_at_s=(pbse+'\actor_target')


            
            

        
        x=xp
        y=yp
        if decay_B:
            if ((ep+t/tfinal)/(epmax/2))<1:
                ss.Hu=Hu0*(1-((ep+t/tfinal)/(epmax/2))*0.9)
            

        
    rvg_cat.append(np.mean(rvg))
    print("RVG",np.mean(rvg))
    if ep%2==0 and save_loss:
        # timestamp=(ep)/epmax*100
        # pfile_L=pbse + '\T' + str(timestamp)+ '.pkl'
        pfile_L=pbse + '\Loss.pkl'

        Loss=[Agent.p_L,Agent.c_L,rcat]
        with open(pfile_L, "wb") as pkl_wb_obj:
            pickle.dump(Loss,pkl_wb_obj)


        # print('f')

# ltR_=time.time()
print('-------------------------------------------------')
print('-------------------------------------------------')
print('-------------------------------------------------')
print('TOTAL TIME:', (time.time()-ltR_)/60, ' min')
print('     Or    ', (time.time()-ltR_)/3600, ' hr')
# %% Loss and reward
loss1=Agent.c_L
loss2=Agent.p_L
Ml1=mov_mean(loss1,65*5)
Ml2=mov_mean(loss2,65*5)
    
rvg=mov_med(rcat,65*3)

plt.figure()
plt.plot(Agent.c_L)
plt.plot(Ml1)
plt.title('Critic')
plt.figure()
plt.plot(Agent.p_L)
plt.plot(Ml2)
plt.title('Actor')

plt.figure()
plt.plot(rcat)
plt.plot(rvg)

plt.figure()
plt.hist(Agent.RB.catalog)
# %% state

plt.figure()
plt.plot(ycat)
plt.title('states')

plt.figure()
plt.plot(ucat)
plt.title('actions')

print(np.mean(rvg_cat))
print(np.sum(rcat))

# %% noise and action normalized

plt.figure()
plt.plot(ucat_n)
plt.figure()
plt.plot(ncat)


# %% 10 last episode

plt.figure()
plt.plot(xcat[tfinal*-10:-1])
# plt.xlim([tfinal*-10,-1])
plt.figure()
plt.plot(ucat[tfinal*-10:-1])

# %% loss find

# loss find
aaa=np.array(Ml1)
print(aaa[-1])
bbb=np.where(aaa<=aaa[-1]*1.1)[0][0:100]
print(bbb)

# %% save data


sb=Agent.RB.state_buffer
ab=Agent.RB.action_buffer
rb=Agent.RB.reward_buffer
ssb=Agent.RB.next_state_buffer

# sb2=np.concatenate(sb).squeeze(1)
# ab2=np.concatenate(ab).squeeze(1)
# rb2=np.concatenate(rb).squeeze(1)
# ssb2=np.concatenate(ssb).squeeze(1)

# buffile=np.concatenate((sb2,ab2,rb2,ssb2),1)

buffile=np.concatenate((sb,ab,rb,ssb),1)

# nam_con="ep_100_12202024_siso"
# nam_con="ep_1000_12202024_siso"

# nam_con="ep_500_01122025_miso_good"
# nam_con="ep_500_01122025_miso_ok"

# nam_con="ep_500_01132025_special_train"
# nam_con="ep_500_01132025_special_train_standard"


# nam_con="ep_500_01132025_loadbuff_special_train"
# nam_con="ep_500_01132025_loadbuff_special_train_standard"



# nam_con="ep_500_01132025_reward_focus_special_train"
# nam_con="ep_500_01132025_reward_focus_special_train_strandard"
# nam_con="ep_500_01182025_special exploration"


# nam_con="ep_500_01212025_standard_10grp"
# nam_con="ep_500_01212025_reward_10grp"
# nam_con="ep_500_01212025_explore_10grp"
# nam_con="ep_500_01212025_both_10grp"


datfile=[xcat, ycat, ucat, rcat, rvg_cat, loss1,loss2]

if save_data:
    pfile=(r"G:\My Drive\Python Scripts\conf_set\data\buffile_"+nam_con+".pkl")
    with open(pfile, "wb") as pkl_wb_obj:
        pickle.dump(buffile,pkl_wb_obj)
    
    pfile=(r"G:\My Drive\Python Scripts\conf_set\data\datfile_"+nam_con+".pkl")
    with open(pfile, "wb") as pkl_wb_obj:
        pickle.dump(datfile,pkl_wb_obj)

#     pfile2=(r"G:\My Drive\Python Scripts\VMPC\vmpc_new_set_102124\data\setfile_"+nam_con+".pkl")
#     with open(pfile2, "wb") as pkl_wb_obj:
#         pickle.dump(setfile,pkl_wb_obj)
    



# %%Open buffer for load data testing

# %% Start Here for test plotting
# nc=8
# end=12000
# data=Agent.RB.data
# data2=Agent.RB.data[:,0:3] #no rewards
# data2_full=Agent.RB.data[:,0:3] #no rewards
# data3=Agent.RB.data[:,0:4]  #rewards
# data3_full=Agent.RB.data[:,0:4]  #rewards
# GnR=mixture.GaussianMixture(n_components=nc, covariance_type="full")
# GnR.fit(data2)
# # GR=mixture.GaussianMixture(n_components=6, covariance_type="tied")
# GR=mixture.BayesianGaussianMixture(n_components=6, covariance_type="tied", max_iter=1000 ) #init_params="random"
# GR.fit(data3)
# # full tied speherical diag

# %%  Start Here for test plotting of trained data
if special_exp or special_train:
    data=Agent.RB.data
    data2=Agent.RB.data[:,0:3] #no rewards
    data2_full=Agent.RB.data[:,0:3] #no rewards
    data3=Agent.RB.data[:,0:4]  #rewards
    data3_full=Agent.RB.data[:,0:4]  #rewards
    GnR=Agent.RB.exp
    GR=Agent.RB.clf
else:
    print("Unintialized GMM")


# %%
tempR=[]
tempA=[]
        
for i in range(GnR.means_.shape[0]):
    # plt.plot()
    dsub =data3[GnR.predict(data2) == i]
    # plt.scatter(dsub[:, 0], dsub[:, 2], color=color, marker="x")
    tempR.append([i,np.mean(dsub[:,3]),np.std(dsub[:,3])])
    tempA.append([i,np.mean(dsub[:,2]),np.std(dsub[:,2])])



K=[]
Entropy=[]
ExpR=[]
B1=GnR.predict_proba(data2)
for i in B1:
    # print(i.shape)
    # print(np.max(i))
    k2=np.max(i)
    H=0
    R_=0
    for grp,s in enumerate(i):
        
        R_=R_+s*tempR[grp][1]
        if s <1e-6:
            H=H+0
        else:
            H=H-s*np.log2(s)
    Entropy.append(H)
    ExpR.append(R_)
    # B1.shape
    # np.append(K,kkk)
    K.append(k2)

test=np.concatenate((data2,np.array([ExpR]).T),1)
# plt.hist(K)
# plt.hist(Entropy)

out=GR.predict(test)
outT=GR.predict(data3)

plt.figure()
plt.plot(data3_full[:,3])
plt.plot(test[:,3])
plt.title("Expect reward based of GnR vs True")

plt.figure()
plt.hist(Entropy)
plt.title("Entropy from GnR")



# %%
tprb=GR.predict_proba(test)

num=[]
Entropy=[]
ExpR=[]

be1=[]
be2=[]
be3=[]
ce1=[]
ce2=[]
ce3=[]

count=[]
count2=[]
for i,k in enumerate(tprb):
    H=0
    R_=0
    for grp,s in enumerate(k):
        # R_=R_+s*tempR[grp][1]
        if s <1e-6:
            H=H+0
        else:
            H=H-s*np.log2(s)
    Entropy.append(H)
    ExpR.append(R_)
    num.append(out[i]-outT[i])
    if out[i]-outT[i]==0:
        be1.append(out[i]-outT[i])
        be2.append(H)
        be3.append(R_)
        count2.append(i)
    else:
        ce1.append(out[i]-outT[i])
        ce2.append(H)
        ce3.append(R_)
        count.append(i)
        

print(len(be1)/len(test))

# %%

plt.figure()
plt.hist(be2, color="C1", label="match")
plt.hist(ce2, color="C0", label="no match")


# plt.hist(be2, color="C1", label="match")
plt.title('entropy if match vs no match')
plt.legend()

plt.figure()
plt.plot(data3_full[:,3])
plt.plot(ExpR)
plt.title('rewards expected vs actual')

# %% skip
# dontr agree
EvR=[]
EvR2=[]
for i,k in enumerate(ce2):
    # if k>np.mean(ce2):
    if k<.1:
        EvR.append(ce3[i])
        EvR2.append(data3[count[i],3])

# plt.figure()
# plt.plot(EvR2)
# plt.plot(EvR)

print(np.mean(abs(np.array(EvR)-np.array(EvR2))))

plt.figure()
plt.hist(EvR2)

# %% skip
# Agree
EvR=[]
EvR2=[]
for i,k in enumerate(be2):
    # if k>np.mean(ce2):
    # if 0.001<k<0.1:
    if k<0.01:
    # if k<0.1:
        EvR.append(be3[i])
        EvR2.append(data3[count2[i],3])

# plt.figure()
# plt.plot(EvR2)
# plt.plot(EvR)

print(np.mean(abs(np.array(EvR)-np.array(EvR2))))
plt.figure()
plt.hist(EvR2)
# %% skip
# Dont agree
EvR=[]
EvR2=[]
for i,k in enumerate(ce2):
    # if k>np.mean(ce2):
    if k>0.99:
        EvR.append(ce3[i])
        EvR2.append(data3[count[i],3])

# plt.figure()
# plt.plot(EvR2)
# plt.plot(EvR)

print(np.mean(abs(np.array(EvR)-np.array(EvR2))))
plt.figure()
plt.hist(EvR2)
# %% skip
# Agree
EvR=[]
EvR2=[]
for i,k in enumerate(be2):
    # if k>np.mean(be2):
    if k>0.99:
        EvR.append(be3[i])
        EvR2.append(data3[count2[i],3])

# plt.figure()
# plt.plot(EvR2)
# plt.plot(EvR)

print(np.mean(abs(np.array(EvR)-np.array(EvR2))))
plt.figure()
plt.hist(EvR2)
# %% skip

EvR=[]
EvR2=[]
for i,k in enumerate(Entropy):
    # if k>np.mean(be2):
    if k>0.5 and k<0.9:
        EvR.append(ExpR[i])
        EvR2.append(data3[i,3])

# plt.figure()
# plt.plot(EvR2)
# plt.plot(EvR)

print(np.mean(abs(np.array(EvR)-np.array(EvR2))))







# %% NEXT
Entropy=[]
ExpR=[]
B1=GR.predict_proba(data3_full)
for i in B1:
    # print(i.shape)
    # print(np.max(i))
    k2=np.max(i)
    H=0
    R_=0
    for grp,s in enumerate(i):
        
        R_=R_+s*tempR[grp][1]
        if s <1e-6:
            H=H+0
        else:
            H=H-s*np.log2(s)
    Entropy.append(H)
    ExpR.append(R_)
    # B1.shape
    # np.append(K,kkk)
    K.append(k2)

# test=np.concatenate((data2,np.array([ExpR]).T),1)
# plt.hist(K)


# out=GR.predict(test)
# outT=GR.predict(data3_full)

plt.figure()
plt.hist(K)
plt.figure()
plt.hist(Entropy)
plt.title("entropy gr")

# %%
tempR=[]
tempA=[]
EntCat=[]
ExpRCat=[]    
for i in range((GR.means_.shape[0])):
    # plt.plot()
    dsub =data3[GR.predict(data3_full) == i]
    # plt.scatter(dsub[:, 0], dsub[:, 2], color=color, marker="x")
    tempR.append([i,np.mean(dsub[:,3]),np.std(dsub[:,3]),len(dsub)])
    # tempA.append([i,np.mean(dsub[:,2]),np.std(dsub[:,2])])
    
    
for i in range((GR.means_.shape[0])):
    dsub =data3[GR.predict(data3_full) == i]
    B1=GR.predict_proba(dsub)
    Entropy=[]
    ExpR=[]
    for f in B1:
        
        k2=np.max(f)
        H=0
        R_=0
        for grp,s in enumerate(f):
            
            R_=R_+s*tempR[grp][1]
            if s <1e-6:
                H=H+0
            else:
                H=H-s*np.log2(s)
        Entropy.append(H)
        ExpR.append(R_)
        # B1.shape
        # np.append(K,kkk)
        K.append(k2)
    EntCat.append([Entropy,i])
    ExpRCat.append([ExpR,i])

# print(tempR)
# %%
for k,i in enumerate(EntCat):
    plt.figure()
    plt.hist(i[0])
    plt.title(i[1])
    plt.figure()
    plt.hist(ExpRCat[k][0], color="C1")
    plt.title(i[1])

# for i in ExpRCat:
#     plt.figure()
#     plt.hist(i[0])
#     plt.title(i[1])


# %%


colors=["blue","orange","green","red","purple","black","yellow","cyan","grey",
        "brown","coral","beige", "skyblue","violet","crimson","orchid"]
plt.figure()
plt.xlabel('state')
plt.ylabel('action')
tempR=[]
tempa=[]
for i, color in enumerate(colors):
    # plt.plot()
    dsub = data3_full[GR.predict(data3_full) == i]
    plt.scatter(dsub[:, 1], dsub[:, 2], color=color, marker="x")
    tempR.append([color,np.mean(dsub[:,3]),dsub[:,3]])
    tempa.append([color,np.mean(dsub[:,2]),dsub[:,2]])
    print(color)
    print(np.mean(dsub[:,3]))
    print(np.std(dsub[:,3]))
    print('--------------------------------------------')
# %%
